behaviors:
  mlagent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048           # Increased from 1024 for better gradient estimates
      buffer_size: 20480         # 10x batch size
      learning_rate: 5.0e-4      # Higher rate for constrained action space
      learning_rate_schedule: linear
      beta: 3.0e-3              # Increased entropy for exploration
      epsilon: 0.15             # Smaller for more stable updates
      lambd: 0.92               # Slightly lower for faster credit assignment
      num_epoch: 4              # More updates per batch

    network_settings:
      normalize: true           # Critical for mixed observation ranges
      hidden_units: 384         # Reduced from 512 (sufficient for constrained space)
      num_layers: 2             # Shallower network
      vis_encode_type: simple
      memory:                   # Added for temporal awareness
        sequence_length: 32
        memory_size: 64

    reward_signals:
      extrinsic:
        gamma: 0.995           # Longer horizon for multi-step poses
        strength: 1.0

    max_steps: 2_000_000       # Extended training
    time_horizon: 256           # Optimal for ~1000 max steps
    summary_freq: 10_000        # Log every 10k steps
    threaded: true              # Enable parallel envs if available

    self_play:                 # Optional for pose diversity
      save_steps: 20_000
      team_change: 100_000
      swap_steps: 5_000
      play_against_latest_model_ratio: 0.3

    behavioral_cloning:        # Ready for demo data
      demo_path: None
      strength: 0.8
      steps: 15_000